# backend/Dockerfile.worker
# Celery worker image (can reuse base API image logic)
FROM python:3.11-slim
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1

# Create non-root user for security
RUN addgroup --system app && adduser --system --ingroup app appuser

WORKDIR /worker

COPY requirements.txt ./requirements.txt
RUN pip install --upgrade pip && pip install -r requirements.txt

COPY . .

# Ensure ownership for runtime writable paths (logs, HuggingFace cache, etc.)
RUN mkdir -p /shared_uploads /worker/logs /worker/.cache && chown -R appuser:app /shared_uploads /worker/logs /worker /worker/.cache

ENV USE_CELERY=true
ENV CELERY_BROKER_URL=${CELERY_BROKER_URL}
ENV CELERY_RESULT_BACKEND=${CELERY_RESULT_BACKEND}
ENV SHARED_UPLOAD_ROOT=/shared_uploads
ENV HOME=/worker
ENV HF_HOME=/worker/.cache/huggingface

# Switch to non-root user
USER appuser

# Command to start celery worker; --pool=solo (single process). Remove --pool to enable multiprocessing.
# Example for multi-process: celery -A app.celery_app.celery_app worker --loglevel=info --concurrency=4
# Use python -Xfrozen_modules=off for more reliable breakpoints.
CMD ["python", "-Xfrozen_modules=off", "-m", "celery", "-A", "app.celery_app.celery_app", "worker", "--loglevel=info", "--pool=solo"]
